{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmmagana/ml-in-production/blob/main/w1_deploying_a_ml_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay6xIXitN7VA"
      },
      "source": [
        "# Ungraded Lab Part 1 - Deploying a Machine Learning Model\n",
        "\n",
        "Welcome to this ungraded lab!\n",
        "\n",
        "This lab is all about seeing how it feels to deploy a real machine learning model. You will deploy a computer vision model trained to detect common objects in images. Typically, deploying the model is one of the last steps in a machine learning lifecycle. However, we thought it would be exciting to get you to deploy a model right away. This lab employs a pretrained object detection model called `YOLOV3`. This model is very convenient for two reasons: it runs really fast, and it yields accurate results.\n",
        "\n",
        "In this lab, you will:\n",
        "1. Inspect the image data set used for object detection\n",
        "2. Take a look at the model itself\n",
        "3. Deploy the model using FastAPI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM-wI4PLN7VD"
      },
      "source": [
        "## Why deployment?\n",
        "\n",
        "It's great to have a model working in a notebook. In practice, though, you will get much more use out of your model when its predictions can be accessed quickly and easily by others who may not have the technical know-how to interact with a notebook. Deploying your object detection model transforms it from a prototype into a practical tool that can be integrated into applications, websites, and other systems. This means your hard work can actually be put to use in real-world scenarios.\n",
        "\n",
        "For instance, deploying the YOLOV3 model would allow developers on your team to incorporate its functionality into a mobile app or surveillance system. Users could then benefit from real-time object detection, whether it's for identifying products in a shopping app or enhancing their security."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdHS8npNN7VD"
      },
      "source": [
        "## Why FastAPI?\n",
        "\n",
        "With FastAPI you can create web servers to host your models very easily, without having to build a complete web application or write any boilerplate. FastAPI handles the complex work behind the scenes, allowing you to write robust apps with straightforward code. Additionally, FastAPI is, well, very fast and it **has a built-in client that can be used to interact with the server**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTXRv76XN7VD"
      },
      "source": [
        "## Object Detection with YOLOV3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX8IZv_9N7VE"
      },
      "source": [
        "### Inspecting the images\n",
        "\n",
        "In this section, you'll use ðŸ”—[`YOLOV3`](https://pjreddie.com/darknet/yolo/) for an object detection task.\n",
        "\n",
        "Let's take a look at the images that will be passed to the YOLOV3 model. Scanning the images will help you form an intuition about what types of objects will be detected. These images are part of the ðŸ”—[`ImageNet`](http://www.image-net.org/index) dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_slqJUIlN7VE"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l-tdotVwN7VF",
        "outputId": "d585f526-4f48-4940-868d-58b6980e2017",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Displaying image: apple.jpg\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'images/apple.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8a7aec70b8b5>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nDisplaying image: {image_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"images/{image_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1232\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'images/apple.jpg'"
          ]
        }
      ],
      "source": [
        "# Some example images\n",
        "image_files = [\n",
        "    'apple.jpg',\n",
        "    'clock.jpg',\n",
        "    'oranges.jpg',\n",
        "    'car.jpg'\n",
        "]\n",
        "\n",
        "for image_file in image_files:\n",
        "    print(f\"\\nDisplaying image: {image_file}\")\n",
        "    display(Image(filename=f\"images/{image_file}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTqQl-TPN7VF"
      },
      "source": [
        "### Overview of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCl3pugLN7VF"
      },
      "source": [
        "\n",
        "Now that you have a sense of the image data you're working with, you will check how accurately the model can detect and classify them.\n",
        "\n",
        "For this task, you will use ðŸ”—[`cvlib`](https://github.com/arunponnusamy/cvlib), which is a very simple but powerful library for object detection that is fueled by ðŸ”—[`OpenCV`](https://docs.opencv.org/4.5.1/) and ðŸ”—[`Tensorflow`](https://www.tensorflow.org/).\n",
        "\n",
        "You will use the ðŸ”—[`detect_common_objects`](https://github.com/arunponnusamy/cvlib/tree/master/docs#object-detection) function from `cvlib`, which takes an image formatted as a ðŸ”—[`numpy array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html) and returns the code below.\n",
        "\n",
        "- `bbox`: list of lists containing bounding box coordinates for detected objects.\n",
        "\n",
        "        Example:\n",
        "    \n",
        "    ```python\n",
        "        [[32, 76, 128, 192], [130, 83, 220, 185]]\n",
        "    ```\n",
        "    \n",
        "\n",
        "- `label`: list of labels for detected objects.\n",
        "    \n",
        "        Example:\n",
        "    ```python\n",
        "        ['orange', 'orange']\n",
        "    ```\n",
        "\n",
        "\n",
        "- `conf`: list of confidence scores for detected objects. A confidence score reflects the model's certainty that the object is really in the image.\n",
        "        Example:\n",
        "        \n",
        "    ```python\n",
        "        [0.6187325716018677, 0.42835739254951477]\n",
        "    ```\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw0l7hXdN7VF"
      },
      "source": [
        "Here's what a saved image with bounding boxes and labels might look like. It's important to understand the expected output from your model when deploying, so you can better communicate with your users.\n",
        "\n",
        "![two oranges detected by YOLOV3](https://github.com/jmmagana/ml-in-production/blob/main/images/oranges_with_bbox.jpeg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB_6CCH2N7VF"
      },
      "source": [
        "### Creating the detect_and_draw_box function\n",
        "\n",
        "Next, you'll create a function that will allow users to interact with your model when it is deployed, called `detect_and_draw_box`. When a user needs to have the model detect objects in an image, FastAPI will use this function.\n",
        "\n",
        "First, create a directory where you can store the resulting images:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uJxy8FlN7VG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "dir_name = \"images_with_boxes\"\n",
        "if not os.path.exists(dir_name):\n",
        "    os.mkdir(dir_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQYGdLbKN7VG"
      },
      "source": [
        "Let's define the `detect_and_draw_box` function which takes as input arguments: the **filename** of a file on your system, a **model**, and a **confidence level**. With these inputs, it detects common objects in the image and saves a new image displaying the bounding boxes alongside the detected object.\n",
        "\n",
        "This function receives the model as an input argument because you have options for which model to use. Going forward, you should use `yolov3-tiny`, a model designed for constrained environments. This model is less accurate than the full model, but still works pretty well. Downloading its pretrained weights takes less time.\n",
        "\n",
        "Recall that the model's output is a vector of probabilities representing the model's confidence that a particular object is in the image at a particular location. The function will use its last input argument, confidence level, to determine the threshold that the probability needs to surpass to report that a given object is detected in the supplied image. By default, `detect_common_objects` uses a value of 0.5. That is, if the model is more than 50% confident in the location of an object in the image, it will draw the bounding box in the newly saved image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPSdtv0qN7VG"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# suppress Tensorflow warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "import cvlib as cv\n",
        "from cvlib.object_detection import draw_bbox\n",
        "\n",
        "\n",
        "def detect_and_draw_box(filename, model=\"yolov3-tiny\", confidence=0.5):\n",
        "    \"\"\"Detects common objects on an image and creates a new image with bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Filename of the image.\n",
        "        model (str): Either \"yolov3\" or \"yolov3-tiny\". Defaults to \"yolov3-tiny\".\n",
        "        confidence (float, optional): Desired confidence level. Defaults to 0.5.\n",
        "    \"\"\"\n",
        "\n",
        "    # Images are stored under the images/ directory\n",
        "    img_filepath = f'images/{filename}'\n",
        "\n",
        "    # Read the image into a numpy array\n",
        "    img = cv2.imread(img_filepath)\n",
        "\n",
        "    # Perform the object detection\n",
        "    bbox, label, conf = cv.detect_common_objects(img, confidence=confidence, model=model)\n",
        "\n",
        "    # Print current image's filename\n",
        "    print(f\"========================\\nImage processed: {filename}\\n\")\n",
        "\n",
        "    # Print detected objects with confidence level\n",
        "    for l, c in zip(label, conf):\n",
        "        print(f\"Detected object: {l} with confidence level of {c}\\n\")\n",
        "\n",
        "    # Create a new image that includes the bounding boxes\n",
        "    output_image = draw_bbox(img, bbox, label, conf)\n",
        "\n",
        "    # Save the image in the directory images_with_boxes\n",
        "    cv2.imwrite(f'images_with_boxes/{filename}', output_image)\n",
        "\n",
        "    # Display the image with bounding boxes\n",
        "    display(Image(f'images_with_boxes/{filename}'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t71P2sION7VG"
      },
      "source": [
        "Let's try it out for the example images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SWVg0ZhN7VG"
      },
      "outputs": [],
      "source": [
        "for image_file in image_files:\n",
        "    detect_and_draw_box(image_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sktOgA3UN7VG"
      },
      "source": [
        "## Changing the confidence level\n",
        "\n",
        "Looks like the object detection went fairly well. Let's try it out on a more difficult image containing several objects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbw3pPkHN7VH"
      },
      "outputs": [],
      "source": [
        "detect_and_draw_box(\"fruits.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6z0AckHN7VH"
      },
      "source": [
        "The **model failed to detect** several fruits and **misclassified** an orange as an apple. This might seem strange since it was able to detect one apple before, so one might think the model has a fair representation on how an apple looks like.\n",
        "\n",
        "One possibility is that the model **did** detect the other fruits but with a confidence level lower than 0.5. Let's test if  this is a valid hypothesis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIkjQH4nN7VH"
      },
      "outputs": [],
      "source": [
        "detect_and_draw_box(\"fruits.jpg\", confidence=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h_iCpVZN7VH"
      },
      "source": [
        "By lowering the confidence level, the model successfully detected most of the fruits. However, you can see that some of them are misclassified (e.g. the label says \"orange\" but it's actually an apple or lemon). This is usually the case when the confidence level is really low. In general, you should be careful when altering parameters like this one, as changing them might yield undesired results. Be mindful of the type of environment where you will be deploying your model: is a quick but inaccurate predication more desirable, or is accuracy more important?\n",
        "\n",
        "As for this concrete example when an orange was misclassified as an apple, it serves as a reminder that these models are not perfect and this should be considered when using them for tasks in production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfJegshhN7VH"
      },
      "source": [
        "## Deploying the model using FastAPI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiSSAWlpN7VH"
      },
      "source": [
        "### Placing your object detection model in a server\n",
        "\n",
        "Now that you know how the model works it is time for you to deploy it! Aren't you excited? :)\n",
        "\n",
        "Before diving into deployment, let's quickly recap some important concepts and how they translate to `FastAPI`. ðŸ”—[You can access the FastAPI docs here.](https://fastapi.tiangolo.com/) Let's also create a directory to store the images that are uploaded to the server.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kfqXsPwN7VH"
      },
      "outputs": [],
      "source": [
        "dir_name = \"images_uploaded\"\n",
        "if not os.path.exists(dir_name):\n",
        "    os.mkdir(dir_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKh_K26YN7VH"
      },
      "source": [
        "### Review: client-server model, HTTP requests, endpoints\n",
        "\n",
        "Below, you'll find review sections for the client-server model, HTTP requests, and endpoints. Please review these as necessary skipping any sections you feel comfortable with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HmP_f_HN7VI"
      },
      "source": [
        "#### Client-Server model\n",
        "\n",
        "**Deployment** typically means putting all of the software required for predicting in a `server`. By doing so, a `client` can interact with the model by sending `requests` to the server. The `server` is similar to a waiter at a restaurant who brings the `client` what they order. The `client` will make `requests` to the `server`, who can respond to those requests with a defined set of options, like bringing food, explaining the menu, or even denying the request if s/he can't complete it.\n",
        "\n",
        "This full client-server interaction is out of the scope of this notebook but, there are a few key concepts you should know.\n",
        "\n",
        "Specifically, the Machine Learning model lives in a server waiting for clients to submit prediction requests. The client should provide the required information that the model needs in order to make a prediction. It's common to batch many predictions in a single request. The server will use the information provided to return predictions to the client, who can then use them at their leisure. The client communicates with the server using its `API`, a defined way of interacting with its code.\n",
        "\n",
        "Here's an example. In the next section, you'll get started by creating an instance of the `FastAPI` class:\n",
        "\n",
        "```python\n",
        "app = FastAPI()\n",
        "```\n",
        "\n",
        "Then, you'll use this instance to create endpoints that handle the prediction logic (see more info about endpoints below). Once all the code is in place to run the server, you only need to use the command:\n",
        "\n",
        "```python\n",
        "uvicorn.run(app)\n",
        "```\n",
        "\n",
        "Your API is coded using FastAPI, but the serving is done using ðŸ”—[`uvicorn`](https://www.uvicorn.org/), which is a really fast Asynchronous Server Gateway Interface (ASGI) implementation. Both technologies are closely interconnected and you don't need to understand the implementation details. Knowing that uvicorn handles the serving is sufficient for the purpose of this lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Zfb_4MN7VI"
      },
      "source": [
        "#### HTTP Requests\n",
        "\n",
        "The client and the server communicate with each other through a protocol called `HTTP` (hypertext transfer protocol). This communication between client and server defines common actions called **requests** using verbs. Two very common requests are:\n",
        "\n",
        "- `GET` -> Retrieves information from the server.\n",
        "- `POST` -> Provides information to the server, which it uses to respond.\n",
        "\n",
        "If your client does a `GET request`, you will get some information from the server without the need to provide additional information. In the case of a `POST request`, you are explicitly telling the server that you will provide some information for it that must be processed in some way. It's similar to social media: if you're just viewing a page (`GET`), the server doesn't need any extra information; if you're making a post or comment (`POST`), you'll need to tell the server what it is you want to say.\n",
        "\n",
        "Interactions with Machine Learning models living in a server are usually done via a `POST request`, since you need to provide the information that is required to compute a prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eB5NXBDN7VI"
      },
      "source": [
        "#### Endpoints\n",
        "\n",
        "You can host multiple Machine Learning models on the same server. For this to work, you can assign a different `endpoint` to each model so you always know what model is being used. An endpoint is represented by a pattern in the `URL`, and you can think of it as a place your client can send requests to. For example, if you have a website called `myawesomemodel.com` you could have three different models in the following endpoints:\n",
        "\n",
        "- `myawesomemodel.com/count-cars/`\n",
        "- `myawesomemodel.com/count-apples/`\n",
        "- `myawesomemodel.com/count-plants/`\n",
        "\n",
        "Each model would do what the name pattern suggests.\n",
        "\n",
        "With FastAPI, you define an endpoint by first creating a function that will handle all of the logic for that endpoint. Then, you will ðŸ”—[decorate it](https://www.geeksforgeeks.org/decorators-in-python/) with a function that contains information about the HTTP method and URL pattern that will trigger your function.\n",
        "\n",
        "The following example shows how to allow a HTTP GET request for the endpoint `\"/my-endpoint\"`:\n",
        "\n",
        "```python\n",
        "# .get -> HTTP method\n",
        "# \"/my-endpoint\" -> URL pattern\n",
        "@app.get(\"/my-endpoint\")\n",
        "def handle_endpoint():\n",
        "    ...\n",
        "    ...\n",
        "```\n",
        "\n",
        "Here's a POST request at `\"/my-other-endpoint\"`:\n",
        "\n",
        "```python\n",
        "# .post -> HTTP method\n",
        "# \"/my-other-endpoint\" -> URL pattern\n",
        "@app.post(\"/my-other-endpoint\")\n",
        "def handle_other_endpoint(param1: int, param2: str):\n",
        "    ...\n",
        "    ...\n",
        "\n",
        "```\n",
        "\n",
        "For POST requests, the handler function includes parameters because it expects the client to provide some information to it. In this case, the function includes two parameters: an integer and a string.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "631byu9yN7VI"
      },
      "source": [
        "### Spinning up the server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-06YdDhN7VI"
      },
      "source": [
        "You'll now create the server using [FastAPI](https://fastapi.tiangolo.com/) and uvicorn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH_0Rp4QN7VI"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import uvicorn\n",
        "import numpy as np\n",
        "import nest_asyncio\n",
        "from enum import Enum\n",
        "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
        "from fastapi.responses import StreamingResponse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPLCbGKWN7VI"
      },
      "outputs": [],
      "source": [
        "# Assign an instance of the FastAPI class to the variable \"app\".\n",
        "# You will interact with your api using this instance.\n",
        "app = FastAPI(title='Deploying an ML Model with FastAPI')\n",
        "\n",
        "# List available models using Enum for convenience. This is useful when the options are pre-defined.\n",
        "class Model(str, Enum):\n",
        "    yolov3tiny = \"yolov3-tiny\"\n",
        "    yolov3 = \"yolov3\"\n",
        "\n",
        "\n",
        "# By using @app.get(\"/\") you are allowing the GET method to work for the / endpoint.\n",
        "@app.get(\"/\")\n",
        "def home():\n",
        "    return \"Congratulations! Your API is working as expected. Now head over to http://serve/docs\"\n",
        "\n",
        "\n",
        "# This endpoint handles all the logic necessary for the object detection to work.\n",
        "# It requires the desired model and the image in which to perform object detection.\n",
        "@app.post(\"/predict\")\n",
        "def prediction(model: Model, file: UploadFile = File(...)):\n",
        "\n",
        "    # 1. VALIDATE INPUT FILE\n",
        "    filename = file.filename\n",
        "    fileExtension = filename.split(\".\")[-1] in (\"jpg\", \"jpeg\", \"png\")\n",
        "    if not fileExtension:\n",
        "        raise HTTPException(status_code=415, detail=\"Unsupported file provided.\")\n",
        "\n",
        "    # 2. TRANSFORM RAW IMAGE INTO CV2 image\n",
        "\n",
        "    # Read image as a stream of bytes\n",
        "    image_stream = io.BytesIO(file.file.read())\n",
        "\n",
        "    # Start the stream from the beginning (position zero)\n",
        "    image_stream.seek(0)\n",
        "\n",
        "    # Write the stream of bytes into a numpy array\n",
        "    file_bytes = np.asarray(bytearray(image_stream.read()), dtype=np.uint8)\n",
        "\n",
        "    # Decode the numpy array as an image\n",
        "    image = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)\n",
        "\n",
        "\n",
        "    # 3. RUN OBJECT DETECTION MODEL\n",
        "\n",
        "    # Run object detection\n",
        "    bbox, label, conf = cv.detect_common_objects(image, model=model)\n",
        "\n",
        "    # Create image that includes bounding boxes and labels\n",
        "    output_image = draw_bbox(image, bbox, label, conf)\n",
        "\n",
        "    # Save it in a folder within the server\n",
        "    cv2.imwrite(f'images_uploaded/{filename}', output_image)\n",
        "\n",
        "\n",
        "    # 4. STREAM THE RESPONSE BACK TO THE CLIENT\n",
        "\n",
        "    # Open the saved image for reading in binary mode\n",
        "    file_image = open(f'images_uploaded/{filename}', mode=\"rb\")\n",
        "\n",
        "    # Return the image as a stream specifying media type\n",
        "    return StreamingResponse(file_image, media_type=\"image/jpeg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUAoiz6qN7VI"
      },
      "source": [
        "By running the following cell you will spin up the server! See the steps below for interacting with it!\n",
        "\n",
        "**Note:** Running the server blocks all cells from running in this notebook until you manually interrupt the kernel. You can do so by clicking on the `Kernel` tab and then on `Interrupt`. You can also enter Jupyter's command mode by pressing the `ESC` key and tapping the `I` key twice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7CAmPSfN7VI"
      },
      "outputs": [],
      "source": [
        "# Allows the server to be run in this interactive environment\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# This is an alias for localhost which means this particular machine\n",
        "host = \"127.0.0.1\"\n",
        "\n",
        "# Spin up the server!\n",
        "uvicorn.run(app, host=host, port=8000, root_path=\"/serve\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK1W1vMzN7VJ"
      },
      "source": [
        "## Consume your service\n",
        "\n",
        "Normally you will now head over to `http://127.0.0.1:8000/` to see it in action. However, the Coursera environment works a bit differently from a regular computer. Within this environment, you need to interact with the service through the navigation bar. This is found in the upper part of your screen. If you don't see this bar you might need to click on the `Navigate` button first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lycu3m1N7VJ"
      },
      "source": [
        "\n",
        "<table><tr><td><img src='https://github.com/jmmagana/ml-in-production/blob/main/assets/navigate.png?raw=1'></td></tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV0gHECHN7VJ"
      },
      "source": [
        "### Go back to this notebook\n",
        "\n",
        "Before you visit the client UI, you'll want to know how to go back to this server notebook. You have two options:\n",
        "\n",
        "- Click the `Home` button at the left side of the navigation bar.\n",
        "\n",
        "\n",
        "- Type `/lab/tree/server-coursera.ipynb` in the navigation bar and press enter.\n",
        "\n",
        "\n",
        "### Visit the server\n",
        "\n",
        "You can think of `/serve/` as an alias for `http://127.0.0.1:8000/`. With this in mind, type `/serve/` in the navigation bar and press `Enter`.\n",
        "\n",
        "This will take you to the `/` endpoint of the server, which should display the message `Congratulations! Your API is working as expected`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xNPXuUoN7VJ"
      },
      "source": [
        "## Using FastAPI's integrated client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWxGlp8kN7VJ"
      },
      "source": [
        "To actually use your server for image detection, you can leverage the client that comes built-in with FastAPI. You'll need to take the following steps to interact with it. Read the list in full before trying it out:\n",
        "\n",
        "1. Type `/serve/docs` in the navigation bar and press `Enter`.\n",
        "\n",
        "<table><tr><td><img src='https://github.com/jmmagana/ml-in-production/blob/main/assets/serve_docs.png?raw=1'></td></tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V12Yd2k8N7VN"
      },
      "source": [
        "2. Click anywhere on the `/predict` endpoint and more options will become visible:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqksKY2ON7VN"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u5uPNwlN7VN"
      },
      "source": [
        "3. Click on the **Try it out** button to test your server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir7_e0q-N7VN"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWfZe3KvN7VN"
      },
      "source": [
        "4. Choose a model from the **model** field (if you select the full YOLOV3 model the server will be stuck until the weights for this model are downloaded)\n",
        "\n",
        "5. Click the **Choose File** button to **submit an image** from your local file system\n",
        "\n",
        "6. Click on the blue **Execute** button to send an HTTP request to the server.\n",
        "\n",
        "7. **Scroll down to see the response from the server**.\n",
        "\n",
        "Pretty cool, right?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0ksVOqTN7VO"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTWJyHZ2N7VO"
      },
      "source": [
        "8. **Try different images!** You can use the ones we provided with this lab or some of your own. Since the model is using the default confidence level of 0.5 it might not always succeed at detecting some objects. You can also try submitting non-image files to see how the server reacts to them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r34dEexnN7VO"
      },
      "source": [
        "## (Optional) Consuming your model from another client\n",
        "\n",
        "It is awesome that FastAPI allows you to interact with your API through its built-in client. However, you might wonder how you can interact with your API using regular code and not some UI.\n",
        "\n",
        "There is a bonus section which shows how to code a minimal client in Python. This is useful to break down (in a very high level) what FastAPI's client is doing under the hood. You can find a notebook named `client-coursera.ipynb` in the file explorer on the left. Double-click on it to open it a new tab, then follow the instructions. Make sure that your server (`uvicorn.run(app, host=host, port=8000, root_path=\"/serve\")`) is still running above before you switch to that notebook."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}